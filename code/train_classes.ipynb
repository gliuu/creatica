{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes the code behind training our convolutional neural network for recognizing hot dog vs non-hot dogs. We assume that we've already validated and taken a look at the images we are working with. (Check out `validate_images.py` and `visualize_hotdogs.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries and defining paths and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import simplejson\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import Dense, Activation, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we define the data file path and some important constants including the batch size and image size ($299$ x $299$ pixels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths and constants\n",
    "cwd = os.getcwd()\n",
    "data_path = os.path.join(cwd, 'data')\n",
    "#data_path = \"/Users/victorialiu/git/creatica/code/data/\"\n",
    "batch_size = 16\n",
    "TARGET_SIZE = 299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Line Argument Parser\n",
    "We want to be able to run our code from the command line (at least in the `.py` version of this notebook), so we use an argument parser to translate command line arguments. We require a model name when running the script, as well as what we want the regularizer strength to be (default to $0$). As we'll see later, having a model name makes it much easier to compare models. By the end of the Hackathon, we'll compare four different models and select the best one for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse command line arguments\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-m', '--model-name',\n",
    "        help='prefix for file to save trained model to ' +\n",
    "            '(e.g. dense_arch1, conv_regularize05, etc.)',\n",
    "        required=True)\n",
    "    parser.add_argument('-r', '--regularizer-strength',\n",
    "        help='strength of l2 regularization to use',\n",
    "        type=float, default=0.00)\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Pre-processing with InceptionV3 net\n",
    "Next, we do data augmentation in order to \"create\" more data to train from. Data augmentation includes shifting the image in small ways such that the same image can be trained from multiple perspective (i.e. a rotated image of a hot dog is still a hot dog, and now we'll have more training data). We also make sure to normalize the image by dividing by the maximum pixel value of $255$. We also write a helper function to easily call based on whether we are using testing or training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bananas/1569.jpg</td>\n",
       "      <td>bananas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bananas/1555.jpg</td>\n",
       "      <td>bananas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bananas/1582.jpg</td>\n",
       "      <td>bananas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bananas/1596.jpg</td>\n",
       "      <td>bananas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bananas/1597.jpg</td>\n",
       "      <td>bananas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>hotdog/160523.jpg</td>\n",
       "      <td>hotdog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>hotdog/219273.jpg</td>\n",
       "      <td>hotdog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>hotdog/190809.jpg</td>\n",
       "      <td>hotdog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>hotdog/201986.jpg</td>\n",
       "      <td>hotdog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>hotdog/225367.jpg</td>\n",
       "      <td>hotdog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              filename    class\n",
       "0     bananas/1569.jpg  bananas\n",
       "1     bananas/1555.jpg  bananas\n",
       "2     bananas/1582.jpg  bananas\n",
       "3     bananas/1596.jpg  bananas\n",
       "4     bananas/1597.jpg  bananas\n",
       "..                 ...      ...\n",
       "145  hotdog/160523.jpg   hotdog\n",
       "146  hotdog/219273.jpg   hotdog\n",
       "147  hotdog/190809.jpg   hotdog\n",
       "148  hotdog/201986.jpg   hotdog\n",
       "149  hotdog/225367.jpg   hotdog\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_data_augment(rescale=1/255, shear_range = False, zoom_range = False, horizontal_flip = False):\n",
    "    #declare ImageDataGenerator class for augmenting images using shear, zoom, and flips\n",
    "    #normalize with 1./255\n",
    "    return (ImageDataGenerator(\n",
    "            rescale=rescale,\n",
    "            shear_range=shear_range,\n",
    "            zoom_range=zoom_range,\n",
    "            horizontal_flip=horizontal_flip))\n",
    "\n",
    "\n",
    "\n",
    "def dataframe_categories(train_or_test):\n",
    "    #define new data_path to make life easier\n",
    "    data_subpath = os.path.join(data_path, train_or_test)\n",
    "    #initiate lists for dataframe\n",
    "    images_names = []\n",
    "    categories = []\n",
    "    \n",
    "    for category in os.listdir(data_subpath):\n",
    "        #no .DS_Store omg I hate .DS_Store\n",
    "        if category != '.DS_Store':\n",
    "            for image in os.listdir(\n",
    "                os.path.join(data_subpath, category)\n",
    "                ):\n",
    "                #only use jpg\n",
    "                if image[-4:] == '.jpg':\n",
    "                    images_names.append(f'{category}/{image}')\n",
    "                    categories.append(category)\n",
    "    images_names = np.array(images_names)\n",
    "    categories = np.array(categories)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'filename' : images_names,\n",
    "        'class' : categories\n",
    "    })\n",
    "                    \n",
    "    return df\n",
    "\n",
    "dataframe_categories('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(train_or_test):\n",
    "    #augment images\n",
    "    if train_or_test == 'train':\n",
    "        datagen = image_data_augment(shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
    "    else:\n",
    "        datagen = image_data_augment()\n",
    "    \n",
    "        \n",
    "    df = dataframe_categories(train_or_test)\n",
    "    \n",
    "    generator = datagen.flow_from_dataframe(\n",
    "        df,\n",
    "        directory = os.path.join(data_path, train_or_test),\n",
    "        target_size=(TARGET_SIZE, TARGET_SIZE),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "        validate_filenames=False\n",
    "        )\n",
    "    return generator\n",
    "# gen = get_images('train')\n",
    "# a = gen[1273]\n",
    "# a = np.array(a)\n",
    "# a = np.reshape(a, (299, 299, 3))\n",
    "# plt.imshow(a)\n",
    "# gen.filenames[1274]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the effort of saving time training the model (which could take overnight and use up precious time) and making the model as accurate as possible, we use transfer learning, where we piggy-back on a pre-trained CNN model that will extract out important features of our data through the bottleneck in the network. We use InceptionV3, and the standard image pre-processing pipeline will always include feeding it through InceptionV3 first. This will make prediction a little slower, but the accuracy and significantly decreased training time (order of minutes) is so worth it. Lastly, we write function `get_data()` that calls all the previous image-preprocessing helper functions to streamline the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inception():\n",
    "    \"\"\"\n",
    "    inception for transfer learning\n",
    "    \"\"\"\n",
    "    #transfer learning with InceptionV3, a pre-trained cnn\n",
    "    model = InceptionV3(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(TARGET_SIZE, TARGET_SIZE, 3),\n",
    "        classes = 3\n",
    "        )\n",
    "    \n",
    "    for train_or_test in [\n",
    "        'train', \n",
    "        'test'\n",
    "        ]:\n",
    "        generator = get_images(train_or_test)\n",
    "\n",
    "        bottleneck_features = model.predict(generator, len(generator), verbose=1)\n",
    "        \n",
    "        #save model with the bottleneck features\n",
    "        np.savez(f'inception_features_{train_or_test}', features=bottleneck_features)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    #augment images and use inception net\n",
    "    preprocess_inception()\n",
    "    #load training data and define labels, where 0 is hotdog and 1 is nothotdog\n",
    "    train_data = np.load('inception_features_train.npz')['features']\n",
    "\n",
    "    #requires the number of hotdog and nothotdog samples to be the exact same\n",
    "    train_data_type_count = int(len(train_data) / 3)\n",
    "    train_labels = np.array(\n",
    "        [0] * train_data_type_count + \n",
    "        [1] * train_data_type_count + \n",
    "        [2] * train_data_type_count\n",
    "        )\n",
    "\n",
    "    #load testing data and define labels, where 0 is hotdog and 1 is nothotdog\n",
    "    test_data = np.load('inception_features_test.npz')['features']\n",
    "\n",
    "    #requires the number of hotdog and nothotdog samples to be the exact same\n",
    "    test_data_type_count = int(len(test_data) / 3)\n",
    "    test_labels = np.array(\n",
    "        [0] * test_data_type_count + \n",
    "        [1] * test_data_type_count + \n",
    "        [2] * test_data_type_count \n",
    "        )\n",
    "    \n",
    "    # Convert labels to one-hot vectors (probability distributions w/\n",
    "    # probability 1 assigned to the correct label)\n",
    "    train_labels = keras.utils.to_categorical(train_labels)\n",
    "    test_labels = keras.utils.to_categorical(test_labels)\n",
    "    \n",
    "    return (train_data, train_labels, test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "Now, we get to the heart of our machine learning model! After being passed through InceptionV3, the feature-extracted version of the image is passed through our own convolutional network. This CNN was largely taken from [J-Yash's open-source code](https://github.com/J-Yash/Hotdog-Not-Hotdog), but we added a regularizer (`reg_param`), which notably can be inputted from the command line. We also commented the code significantly to clarify what each line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_net(reg_param, train_data_shape):\n",
    "    #train_data_shape = train_data.shape[1:] = (8, 8, 2048)\n",
    "    model = Sequential()\n",
    "    \n",
    "    #convolutional layer with 32 3x3 trainable filters, using rectified linear units.\n",
    "    #Padding to result in the same shape as the original picture. Add reg_param if true\n",
    "    #use l2 regularization if reg_param is given in command line\n",
    "    model.add(Conv2D(\n",
    "        16, (3, 3),\n",
    "        activation='relu',\n",
    "        input_shape=train_data_shape,\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(reg_param)\n",
    "        ))\n",
    "#     # max pooling for noise reduction\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    \n",
    "    # second block\n",
    "    model.add(Conv2D(\n",
    "        32, (3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(reg_param)\n",
    "        ))\n",
    "    # max pooling for noise reduction\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # dropout for more regularization\n",
    "    model.add(Dropout(0.25))\n",
    "     \n",
    "        \n",
    "    # third block\n",
    "    model.add(Conv2D(\n",
    "        64, (3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(reg_param)\n",
    "        ))\n",
    "#     model.add(Conv2D(\n",
    "#         64, (3, 3),\n",
    "#         activation='relu',\n",
    "#         padding='same',\n",
    "#         kernel_regularizer=l2(reg_param)\n",
    "#         ))\n",
    "    # max pooling for noise reduction\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     dropout for more regularization\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    # fourth block\n",
    "    model.add(Conv2D(\n",
    "        128, (3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(reg_param)\n",
    "        ))\n",
    "#     model.add(Conv2D(\n",
    "#         128, (3, 3),\n",
    "#         activation='relu',\n",
    "#         padding='same',\n",
    "#         kernel_regularizer=l2(reg_param)\n",
    "#         ))\n",
    "#     # max pooling for noise reduction\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # dropout for more regularization\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    #fifth block\n",
    "    model.add(Conv2D(\n",
    "        256, (3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(reg_param)\n",
    "        ))\n",
    "    model.add(Conv2D(\n",
    "        256, (3, 3),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(reg_param)\n",
    "        ))\n",
    "#     model.add(Conv2D(\n",
    "#         256, (3, 3),\n",
    "#         activation='relu',\n",
    "#         padding='same',\n",
    "#         kernel_regularizer=l2(reg_param)\n",
    "#         ))\n",
    "    # max pooling for noise reduction\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # dropout for more regularization\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "\n",
    "    # fully connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    #4 categories\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2508 non-validated image filenames belonging to 3 classes.\n",
      "157/157 [==============================] - 98s 626ms/step\n",
      "Found 150 non-validated image filenames belonging to 3 classes.\n",
      "10/10 [==============================] - 5s 490ms/step\n"
     ]
    }
   ],
   "source": [
    "# Importing the hotdog dataset\n",
    "#may take a few seconds\n",
    "(train_data, train_labels, test_data, test_labels) = get_data()\n",
    "train_data_shape = train_data.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready to go\n",
    "The `main()` function is mainly for the command line, but it can also be run from Jupyter if the right lines are commented out. Notably, we use the Adam optimizer, we calculate loss based on categorical cross-entropy, and we save the accuracy to plot. Notably, we also have a checkpointer, so that we only save the best model after multiple epochs. The best model is defined the model in which the validation error is the lowest, not the training error. We have a 30/70% training vs. validation split, which is generally a good rule of thumb, and we choose $8$ epochs since the accuracy generally peaks within the first $5$ epochs... The saved model is saved as a `.h5` and `.json` file under `creatica/code/model`, so that we can load it in another module. Finally, we output a graph of the loss and accuracy metrics, compared between training and validation sets. The image is actually saved to the `/creatica/code/img` folder, but it also pops up when the script is run from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_data, train_labels, test_data, test_labels):\n",
    "    \n",
    "# comment this out when running from command line!\n",
    "    model_name = 'multiclass_debugging'\n",
    "    regularizer_strength = .00001\n",
    "\n",
    "# #     comment out when not running from cmdline\n",
    "#     ## get cmdline args\n",
    "#     args = parse_args()\n",
    "#     model_name = args.model_name\n",
    "#     # get regularization strength, if defined. Otherwise, it is 0\n",
    "#     regularizer_strength = args.regularizer_strength\n",
    "\n",
    "\n",
    "    # Remove src from cwd if necessary\n",
    "    cwd = os.getcwd()\n",
    "    if os.path.basename(cwd) == 'src': cwd = os.path.dirname(cwd)\n",
    "\n",
    "    # Create img directory to save images if needed\n",
    "    os.makedirs(os.path.join(cwd, 'img'), exist_ok=True)\n",
    "    plot_fname = os.path.join(cwd, 'img', '%s_learn.png' % model_name)\n",
    "\n",
    "    # Create model directory to save models if needed\n",
    "    os.makedirs(os.path.join(cwd, 'model'), exist_ok=True)\n",
    "    model_weights_fname = os.path.join(cwd, 'model', model_name + '.h5')\n",
    "    model_json_fname = os.path.join(cwd, 'model', model_name + '.json')\n",
    "\n",
    "\n",
    "#     # Importing the hotdog dataset\n",
    "#     #may take a few seconds\n",
    "#     (train_data, train_labels, test_data, test_labels) = get_data()\n",
    "#     train_data_shape = train_data.shape[1:]\n",
    "\n",
    "    \n",
    "\n",
    "    # build model\n",
    "    model = build_conv_net(regularizer_strength, train_data_shape)\n",
    "    \n",
    "    # Print a summary of the layers and weights in the model\n",
    "    model.summary()\n",
    "\n",
    "    # Have our model minimize the binary cross entropy loss with the adam\n",
    "    # optimizer (fancier stochastic gradient descent that converges faster)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['MSE'])\n",
    "    \n",
    "    # #set checkpointer to use in callback, to only keep the best model weights\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath='/Users/victorialiu/git/creatica/tmp',\n",
    "        verbose=1, \n",
    "        save_weights_only=True,\n",
    "        )\n",
    "\n",
    "    #time to fit\n",
    "    history = model.fit(train_data, train_labels,\n",
    "            epochs=8,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            validation_data=(train_data, train_labels), \n",
    "            verbose=2,\n",
    "            callbacks=[checkpointer],\n",
    "            shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    #load best model\n",
    "    model.load_weights('/Users/victorialiu/git/creatica/tmp')\n",
    "\n",
    "    # Save model weights and json spec describing the model's architecture\n",
    "    model.save(model_weights_fname)\n",
    "    model_json = model.to_json()\n",
    "    with open(model_json_fname, 'w') as f:\n",
    "        f.write(simplejson.dumps(simplejson.loads(model_json), indent=4))\n",
    "\n",
    "        \n",
    "\n",
    "#     # Plot accuracy learning curve\n",
    "#     ax1 = plt.subplot(2, 1, 1)\n",
    "#     plt.plot(history.history['accuracy'])\n",
    "#     plt.plot(history.history['val_accuracy'])\n",
    "#     plt.title('%s accuracy' % model_name)\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "#     plt.savefig(plot_fname)\n",
    "\n",
    "#     # Plot loss learning curve\n",
    "#     plt.subplot(2, 1, 2, sharex=ax1)\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title('%s loss' % model_name)\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Plot accuracy learning curve\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['MSE'])\n",
    "    plt.plot(history.history['val_MSE'])\n",
    "    plt.title('%s accuracy' % model_name)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    plt.savefig(plot_fname)\n",
    "                             \n",
    "    \n",
    "    # Plot loss learning curve\n",
    "    plt.subplot(2, 1, 2, sharex=ax1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('%s loss' % model_name)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_fname)\n",
    "    plt.show()\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "main(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: Victoria Liu and Gloria Liu\n",
    "\n",
    "**Last modified**: November 2020\n",
    "\n",
    "Description: A script to train and save a neural net to recognize hot dogs vs.\n",
    "non-hot dogs.\n",
    "\n",
    "**Credits** Parts of the code are originally part of a Caltech extra credit assignment (CS 156a), where\n",
    "Aadyot Bhatnagar wrote the parse_args() and main() functions. The conv-net code is heavily modified from [J-Yash's open-source code](https://github.com/J-Yash/Hotdog-Not-Hotdog). All markdown is done by Victoria / Gloria.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python",
   "text_representation": {
    "extension": ".py",
    "format_name": "sphinx"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
